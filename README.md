# 218_Sentiment_Analysis
ğŸš€ A comparative study of **three sentiment analysis models** on tweets.

---

## ğŸ“ Project Overview  
This project aims to **classify sentiment** (Positive, Neutral, Negative) using **three different approaches**:

1ï¸âƒ£ **Fine-tuned DistilBERT (distilbert-based-uncased)**  
2ï¸âƒ£ **Fine-tuned XLM-RoBERTa for Bilingual Sentiment Analysis**  
3ï¸âƒ£ **VADER + TF-IDF + Random Forest (Hybrid Approach)**  

**Objective:**  
Compare the **performance, accuracy, and efficiency** of deep learning vs. traditional NLP models for sentiment analysis.

---

## ğŸ“‚ Datasets  
We use two main datasets:

1ï¸âƒ£ **English Sentiment Dataset** (Twitter-based)  
ğŸ“Œ [Kaggle Link](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis)  

2ï¸âƒ£ **Burmese Sentiment Dataset**  
ğŸ“Œ [Hugging Face Link](https://huggingface.co/datasets/kornwtp/burmese-gklmip-sentiment)  

---

## ğŸ“Š Results & Comparison  

| Model | Accuracy | Precision | Recall | F1-Score |
|--------|----------|-----------|--------|---------|
| Fine-tuned XLM-RoBERTa | 86.79% | 0.87 | 0.87 | 0.87 |
| Fine-tuned DistilBERT | 88.15% | 0.88 | 0.88 | 0.88 |
| VADER + TF-IDF + Random Forest | 97% | 0.97 | 0.97 | 0.97 |

ğŸŸ¢ **VADER + TF-IDF struggled with Burmese text** showing strong sentiment classification.  
 
---
## Feel free to request for collaborations â¤ï¸
